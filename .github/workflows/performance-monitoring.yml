name: 'Performance Monitoring & Benchmarking'

on:
  schedule:
    - cron: '0 4 * * 1,3,5' # Mon, Wed, Fri at 4 AM
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'full'
        type: choice
        options: ['full', 'startup', 'ui', 'memory']
  push:
    branches: [main]
    paths:
      - 'app/src/**'
      - 'core-*/**'

env:
  BASELINE_BRANCH: 'main'
  PERFORMANCE_THRESHOLD: '10' # 10% regression threshold

jobs:
  performance-benchmarks:
    name: 'Performance Benchmarks'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    strategy:
      matrix:
        benchmark-type:
          - startup
          - ui-rendering
          - pose-detection
          - memory-usage

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure Git
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Setup Android SDK
        uses: android-actions/setup-android@v3

      - name: Grant execute permission for gradlew
        run: chmod +x gradlew

      - name: Cache Gradle Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.gradle/caches
            ~/.gradle/wrapper
          key: gradle-${{ runner.os }}-${{ hashFiles('**/*.gradle*') }}

      - name: Build Benchmark APK
        run: |
          ./gradlew :app:assembleBenchmark
          ./gradlew :app:assembleAndroidTest

      - name: Setup Performance Monitoring
        run: |
          # Install performance monitoring tools
          sudo apt-get update
          sudo apt-get install -y bc jq

          # Create performance results directory
          mkdir -p performance-results

      - name: Run Startup Benchmarks
        if: matrix.benchmark-type == 'startup' || github.event.inputs.benchmark_type == 'full'
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim
          disable-animations: true
          script: |
            adb install app/build/outputs/apk/benchmark/app-benchmark.apk
            adb install app/build/outputs/apk/androidTest/benchmark/app-benchmark-androidTest.apk

            # Run startup time benchmarks
            adb shell am instrument -w \
              -e class com.posecoach.benchmark.StartupBenchmark \
              com.posecoach.android.benchmark.test/androidx.benchmark.junit4.AndroidBenchmarkRunner \
              > performance-results/startup-benchmark.txt

      - name: Run UI Rendering Benchmarks
        if: matrix.benchmark-type == 'ui-rendering' || github.event.inputs.benchmark_type == 'full'
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim
          disable-animations: true
          script: |
            # Run UI rendering benchmarks
            adb shell am instrument -w \
              -e class com.posecoach.benchmark.UiRenderingBenchmark \
              com.posecoach.android.benchmark.test/androidx.benchmark.junit4.AndroidBenchmarkRunner \
              > performance-results/ui-rendering-benchmark.txt

      - name: Run Pose Detection Benchmarks
        if: matrix.benchmark-type == 'pose-detection' || github.event.inputs.benchmark_type == 'full'
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim
          disable-animations: true
          script: |
            # Run pose detection performance benchmarks
            adb shell am instrument -w \
              -e class com.posecoach.benchmark.PoseDetectionBenchmark \
              com.posecoach.android.benchmark.test/androidx.benchmark.junit4.AndroidBenchmarkRunner \
              > performance-results/pose-detection-benchmark.txt

      - name: Run Memory Usage Benchmarks
        if: matrix.benchmark-type == 'memory-usage' || github.event.inputs.benchmark_type == 'full'
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim
          disable-animations: true
          script: |
            # Run memory usage benchmarks
            adb shell am instrument -w \
              -e class com.posecoach.benchmark.MemoryUsageBenchmark \
              com.posecoach.android.benchmark.test/androidx.benchmark.junit4.AndroidBenchmarkRunner \
              > performance-results/memory-usage-benchmark.txt

      - name: Extract Performance Metrics
        run: |
          echo "üìä Extracting Performance Metrics"
          echo "=================================="

          # Function to extract metrics from benchmark output
          extract_metrics() {
            local file=$1
            local metric_type=$2

            if [ -f "$file" ]; then
              # Extract key metrics (timing, memory, etc.)
              grep -E "median|min|max|mean" "$file" | while read line; do
                echo "$metric_type: $line"
              done >> performance-results/metrics-summary.txt
            fi
          }

          # Extract metrics from all benchmark files
          extract_metrics "performance-results/startup-benchmark.txt" "Startup"
          extract_metrics "performance-results/ui-rendering-benchmark.txt" "UI Rendering"
          extract_metrics "performance-results/pose-detection-benchmark.txt" "Pose Detection"
          extract_metrics "performance-results/memory-usage-benchmark.txt" "Memory Usage"

          # Generate performance summary
          cat > performance-results/performance-summary.md << EOF
          # Performance Benchmark Results

          ## Test Environment
          - **API Level**: 34
          - **Architecture**: x86_64
          - **Benchmark Type**: ${{ matrix.benchmark-type }}
          - **Date**: $(date)
          - **Commit**: ${{ github.sha }}

          ## Results Summary
          \`\`\`
          $(cat performance-results/metrics-summary.txt 2>/dev/null || echo "No metrics available")
          \`\`\`

          ## Benchmark Files
          - Startup: $([ -f "performance-results/startup-benchmark.txt" ] && echo "‚úÖ Available" || echo "‚ùå Not run")
          - UI Rendering: $([ -f "performance-results/ui-rendering-benchmark.txt" ] && echo "‚úÖ Available" || echo "‚ùå Not run")
          - Pose Detection: $([ -f "performance-results/pose-detection-benchmark.txt" ] && echo "‚úÖ Available" || echo "‚ùå Not run")
          - Memory Usage: $([ -f "performance-results/memory-usage-benchmark.txt" ] && echo "‚úÖ Available" || echo "‚ùå Not run")
          EOF

      - name: Compare with Baseline (if available)
        run: |
          # Try to download baseline performance data
          if gh run list --workflow="performance-monitoring.yml" --branch="$BASELINE_BRANCH" --status="success" --limit=1 --json=conclusion,databaseId | jq -r '.[0].databaseId' > /dev/null 2>&1; then
            BASELINE_RUN_ID=$(gh run list --workflow="performance-monitoring.yml" --branch="$BASELINE_BRANCH" --status="success" --limit=1 --json=conclusion,databaseId | jq -r '.[0].databaseId')

            echo "üìà Comparing with baseline run: $BASELINE_RUN_ID"

            # Download baseline artifacts for comparison
            gh run download "$BASELINE_RUN_ID" --name "performance-results-${{ matrix.benchmark-type }}" --dir baseline/ 2>/dev/null || true

            if [ -d "baseline/" ]; then
              echo "‚úÖ Baseline data available for comparison"

              # Simple comparison logic (can be enhanced)
              cat > performance-results/regression-analysis.md << EOF
          # Performance Regression Analysis

          ## Comparison with Baseline
          - **Baseline Run**: $BASELINE_RUN_ID
          - **Current Run**: ${{ github.run_id }}

          ## Analysis
          Detailed regression analysis requires manual review of benchmark outputs.

          ‚ö†Ô∏è **Note**: Automated regression detection is not implemented yet.
          Please manually compare the metrics between runs.
          EOF
            else
              echo "‚ÑπÔ∏è No baseline data available for comparison"
            fi
          else
            echo "‚ÑπÔ∏è No previous successful runs found for baseline comparison"
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Upload Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ matrix.benchmark-type }}
          path: performance-results/
          retention-days: 90

  aggregate-results:
    name: 'Aggregate Performance Results'
    runs-on: ubuntu-latest
    needs: performance-benchmarks
    if: always()
    timeout-minutes: 10

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure Git
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Download All Performance Results
        uses: actions/download-artifact@v4
        with:
          pattern: performance-results-*
          path: all-results/
          merge-multiple: true

      - name: Generate Comprehensive Report
        run: |
          mkdir -p final-report

          cat > final-report/comprehensive-performance-report.md << EOF
          # Comprehensive Performance Report

          ## Executive Summary
          - **Test Date**: $(date)
          - **Commit**: ${{ github.sha }}
          - **Branch**: ${{ github.ref_name }}
          - **Trigger**: ${{ github.event_name }}

          ## Performance Categories Tested
          $(ls all-results/ | grep -E "\\.md$" | while read file; do
            echo "- $(basename "$file" .md | sed 's/-/ /g' | sed 's/\b\w/\U&/g')"
          done)

          ## Detailed Results
          $(for file in all-results/*.md; do
            if [ -f "$file" ]; then
              echo "### $(basename "$file" .md | sed 's/-/ /g' | sed 's/\b\w/\U&/g')"
              echo ""
              cat "$file"
              echo ""
              echo "---"
              echo ""
            fi
          done)

          ## Performance Metrics Archive
          All raw benchmark data is available in the workflow artifacts.

          ## Recommendations
          1. Monitor trends over time for performance regressions
          2. Set up alerting for significant performance degradations
          3. Implement automated baseline comparisons
          4. Consider A/B testing for performance-critical changes

          Generated by: GitHub Actions Performance Monitoring
          EOF

      - name: Check for Performance Regressions
        run: |
          echo "üîç Checking for Performance Regressions"
          echo "======================================="

          # Simple heuristic for detecting potential issues
          POTENTIAL_ISSUES=0

          # Check for any benchmark failures
          if find all-results/ -name "*.txt" -exec grep -l "FAILURE\|ERROR\|Exception" {} \; | head -1; then
            echo "‚ùå Benchmark failures detected"
            POTENTIAL_ISSUES=$((POTENTIAL_ISSUES + 1))
          fi

          # Check for extremely long execution times (basic heuristic)
          if find all-results/ -name "*.txt" -exec grep -l "timeout\|Timeout" {} \; | head -1; then
            echo "‚ö†Ô∏è Potential timeout issues detected"
            POTENTIAL_ISSUES=$((POTENTIAL_ISSUES + 1))
          fi

          if [ $POTENTIAL_ISSUES -eq 0 ]; then
            echo "‚úÖ No obvious performance issues detected"
          else
            echo "‚ö†Ô∏è $POTENTIAL_ISSUES potential performance issues detected"
            echo "Please review the detailed benchmark results"
          fi

          echo "POTENTIAL_ISSUES=$POTENTIAL_ISSUES" >> $GITHUB_ENV

      - name: Upload Comprehensive Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-performance-report
          path: final-report/
          retention-days: 180

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = fs.readFileSync('final-report/comprehensive-performance-report.md', 'utf8');
              const summary = report.split('## Detailed Results')[0] + '\n\nüìä Full performance report available in workflow artifacts.';

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## üìà Performance Benchmark Results\n\n${summary}`
              });
            } catch (error) {
              console.log('Could not post performance report:', error.message);
            }

      - name: Notify on Performance Issues
        if: env.POTENTIAL_ISSUES != '0'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `‚ö†Ô∏è Performance Issues Detected - ${context.sha.substring(0, 7)}`,
              body: `
              ## Performance Alert

              The performance monitoring workflow has detected potential issues:

              - **Commit**: ${context.sha}
              - **Branch**: ${context.ref}
              - **Issues Detected**: ${process.env.POTENTIAL_ISSUES}

              Please review the performance benchmark results and investigate any regressions.

              **Action Items:**
              1. Review benchmark artifacts from run #${context.runNumber}
              2. Compare with baseline performance metrics
              3. Investigate any significant regressions
              4. Consider optimization if needed

              ---
              *Auto-generated by Performance Monitoring Workflow*
              `,
              labels: ['performance', 'investigation-needed']
            });